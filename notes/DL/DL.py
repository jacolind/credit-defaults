# Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow

### wages

## Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential

## Data
# Y: hourly wage
# X: education, experience and age are in years. binary variables: member of a union? married? whether they are from the south. dummies for manufacturing or consutrction
in [1]: df.head()
Out[1]:
            wage  union      education     experience  age  female  married  \
0           5.10      0              8              21   35       1     1
1           4.95      0              9              42   57       1     1
2           6.67      0             12               1   19       0     0
3           4.00      0             12               4   22       0     0
4           7.50      0             12              17   35       0     1

   south  manufacturing  construction
0      0              1             0
1      0              1             0
2      0              1             0
3      0              0             0
4      0              0             0

In [2]: df.describe()
Out[2]:
       wage       union  education_yrs  experience_yrs         age  \
count     534.000000  534.000000     534.000000      534.000000  534.000000
mean        9.024064    0.179775      13.018727       17.822097   36.833333
std         5.139097    0.384360       2.615373       12.379710   11.726573
min         1.000000    0.000000       2.000000        0.000000   18.000000
25%         5.250000    0.000000      12.000000        8.000000   28.000000
50%         7.780000    0.000000      12.000000       15.000000   35.000000
75%        11.250000    0.000000      15.000000       26.000000   44.000000
max        44.500000    1.000000      18.000000       55.000000   64.000000

           female        marr       south  manufacturing  construction
count  534.000000  534.000000  534.000000     534.000000    534.000000
mean     0.458801    0.655431    0.292135       0.185393      0.044944
std      0.498767    0.475673    0.455170       0.388981      0.207375
min      0.000000    0.000000    0.000000       0.000000      0.000000
25%      0.000000    0.000000    0.000000       0.000000      0.000000
50%      0.000000    1.000000    0.000000       0.000000      0.000000
75%      1.000000    1.000000    1.000000       0.000000      0.000000
max      1.000000    1.000000    1.000000       1.000000      1.000000


## Specify
n_cols = predictors.shape[1]
model = Sequential()
shape = (n_cols,)
model.add(Dense(50, activation='relu', input_shape = shape))
model.add(Dense(32, activation='relu'))
model.add(Dense(1))

## Compile
model.compile(optimizer='adam', loss='mean_squared_error')

## Fit
model.fit(predictors, target)

# Output of Fit

        Epoch 1/10

     32/534 [>.............................] - ETA: 0s - loss: 96.0186
    534/534 [==============================] - 0s - loss: 41.6377
        Epoch 2/10

     32/534 [>.............................] - ETA: 0s - loss: 37.6689
    534/534 [==============================] - 0s - loss: 25.4472
        Epoch 3/10

     32/534 [>.............................] - ETA: 0s - loss: 11.4840
    534/534 [==============================] - 0s - loss: 23.4738
        Epoch 4/10

     32/534 [>.............................] - ETA: 0s - loss: 13.7620
    534/534 [==============================] - 0s - loss: 23.0674
        Epoch 5/10

     32/534 [>.............................] - ETA: 0s - loss: 50.3892
    534/534 [==============================] - 0s - loss: 22.4491
        Epoch 6/10

     32/534 [>.............................] - ETA: 0s - loss: 24.0040
    534/534 [==============================] - 0s - loss: 22.6989
        Epoch 7/10

     32/534 [>.............................] - ETA: 0s - loss: 17.4551
    534/534 [==============================] - 0s - loss: 21.7398
        Epoch 8/10

     32/534 [>.............................] - ETA: 0s - loss: 17.8472
    534/534 [==============================] - 0s - loss: 21.6278
        Epoch 9/10

     32/534 [>.............................] - ETA: 0s - loss: 21.8092
    534/534 [==============================] - 0s - loss: 21.8023
        Epoch 10/10

     32/534 [>.............................] - ETA: 0s - loss: 14.3941
    534/534 [==============================] - 0s - loss: 21.2953

#








### titanic
# Now you will start modeling with a new dataset for a classification problem. This data includes information about passengers on the Titanic. You will use predictors such as age, fare and where each passenger embarked from to predict who will survive. This data is from a tutorial on data science competitions. Look here for descriptions of the features.
# https://www.kaggle.com/c/titanic
# https://www.kaggle.com/c/titanic/data


In [2]: df.describe()
Out[2]:
         survived      pclass         age       sibsp       parch        fare  \
count  891.000000  891.000000  891.000000  891.000000  891.000000  891.000000
mean     0.383838    2.308642   29.699118    0.523008    0.381594   32.204208
std      0.486592    0.836071   13.002015    1.102743    0.806057   49.693429
min      0.000000    1.000000    0.420000    0.000000    0.000000    0.000000
25%      0.000000    2.000000   22.000000    0.000000    0.000000    7.910400
50%      0.000000    3.000000   29.699118    0.000000    0.000000   14.454200
75%      1.000000    3.000000   35.000000    1.000000    0.000000   31.000000
max      1.000000    3.000000   80.000000    8.000000    6.000000  512.329200

             male  embarked_from_cherbourg  embarked_from_queenstown  \
count  891.000000               891.000000                891.000000
mean     0.647587                 0.188552                  0.086420
std      0.477990                 0.391372                  0.281141
min      0.000000                 0.000000                  0.000000
25%      0.000000                 0.000000                  0.000000
50%      1.000000                 0.000000                  0.000000
75%      1.000000                 0.000000                  0.000000
max      1.000000                 1.000000                  1.000000

       embarked_from_southampton
count                 891.000000
mean                    0.722783
std                     0.447876
min                     0.000000
25%                     0.000000
50%                     1.000000
75%                     1.000000
max                     1.000000


## Import necessary modules
import keras
from keras.layers import Dense
from keras.models import Sequential
from keras.utils import to_categorical

## Import data
df = pd.read_csv('titatic.csv')
df.head()

    In [3]: df.head(3)
    Out[3]:
       SURVIVED  PCLASS   AGE  SIBSP  PARCH     FARE  MALE          AGE_NA  \
    0         0       3  22.0      1      0   7.2500     1           False
    1         1       1  38.0      1      0  71.2833     0           False
    2         1       3  26.0      0      0   7.9250     0           False


       CHERBOURG  QUEENSTOWN  SOUTHAMPTON
    0  0          0           1
    1  1          0           0
    2  0          0           1


## Target (Y)
target = to_categorical(df['survived'])

## Specify
model = Sequential()
n_cols = df.shape[1] - 1
shape = (n_cols,)
model.add(Dense(32, activation='relu', input_shape=shape)
model.add(Dense(2, activation='softmax'))

## Compile
model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])

## Fit
model.fit(predictors, target)

## Accuracy = 66 %


    Epoch 1/10

     32/891 [>.............................] - ETA: 1s - loss: 6.7778 - acc: 0.3750
    544/891 [=================>............] - ETA: 0s - loss: 3.0149 - acc: 0.5882
    891/891 [==============================] - 0s - loss: 2.9774 - acc: 0.5915
        Epoch 2/10

     32/891 [>.............................] - ETA: 0s - loss: 1.6587 - acc: 0.6562
    480/891 [===============>..............] - ETA: 0s - loss: 2.4059 - acc: 0.5896
    891/891 [==============================] - 0s - loss: 2.1610 - acc: 0.5780
        Epoch 3/10

     32/891 [>.............................] - ETA: 0s - loss: 2.1603 - acc: 0.5625
    544/891 [=================>............] - ETA: 0s - loss: 1.1650 - acc: 0.6452
    891/891 [==============================] - 0s - loss: 1.2803 - acc: 0.6330
        Epoch 4/10

     32/891 [>.............................] - ETA: 0s - loss: 0.7058 - acc: 0.7188
    544/891 [=================>............] - ETA: 0s - loss: 0.9063 - acc: 0.6618
    864/891 [============================>.] - ETA: 0s - loss: 0.8665 - acc: 0.6690
    891/891 [==============================] - 0s - loss: 0.8546 - acc: 0.6734
        Epoch 5/10

     32/891 [>.............................] - ETA: 0s - loss: 0.5330 - acc: 0.7812
    416/891 [=============>................] - ETA: 0s - loss: 0.9267 - acc: 0.6779
    832/891 [===========================>..] - ETA: 0s - loss: 0.8987 - acc: 0.6587
    891/891 [==============================] - 0s - loss: 0.8773 - acc: 0.6644
        Epoch 6/10

     32/891 [>.............................] - ETA: 0s - loss: 0.5146 - acc: 0.7500
    384/891 [===========>..................] - ETA: 0s - loss: 1.0612 - acc: 0.6589
    768/891 [========================>.....] - ETA: 0s - loss: 1.0058 - acc: 0.6302
    891/891 [==============================] - 0s - loss: 0.9547 - acc: 0.6487
        Epoch 7/10

     32/891 [>.............................] - ETA: 0s - loss: 0.6301 - acc: 0.6875
    416/891 [=============>................] - ETA: 0s - loss: 1.0823 - acc: 0.6106
    800/891 [=========================>....] - ETA: 0s - loss: 0.8956 - acc: 0.6350
    891/891 [==============================] - 0s - loss: 1.0245 - acc: 0.6386
        Epoch 8/10

     32/891 [>.............................] - ETA: 0s - loss: 0.6638 - acc: 0.6875
    448/891 [==============>...............] - ETA: 0s - loss: 0.9076 - acc: 0.6451
    832/891 [===========================>..] - ETA: 0s - loss: 0.9542 - acc: 0.6502
    891/891 [==============================] - 0s - loss: 0.9574 - acc: 0.6431
        Epoch 9/10

     32/891 [>.............................] - ETA: 0s - loss: 1.0530 - acc: 0.7188
    544/891 [=================>............] - ETA: 0s - loss: 1.5065 - acc: 0.5901
    891/891 [==============================] - 0s - loss: 1.4952 - acc: 0.6105
        Epoch 10/10

     32/891 [>.............................] - ETA: 0s - loss: 1.7903 - acc: 0.6562
    384/891 [===========>..................] - ETA: 0s - loss: 1.1822 - acc: 0.6693
    576/891 [==================>...........] - ETA: 0s - loss: 1.0954 - acc: 0.6701
    800/891 [=========================>....] - ETA: 0s - loss: 1.1167 - acc: 0.6525
    891/891 [==============================] - 0s - loss: 1.0743 - acc: 0.6566


# qq todo: compare this prediction capacity with merely using logistic regression.







### vet ej vad nedan har för data men det är samma daset i många övningar framöver, akllat predictors och target

predictors :
array([[3, 22.0, 1, ..., 0, 0, 1],
       [1, 38.0, 1, ..., 1, 0, 0],
       [3, 26.0, 0, ..., 0, 0, 1],
       ...,
       [3, 29.69911764705882, 1, ..., 0, 0, 1],
       [1, 26.0, 0, ..., 1, 0, 0],
       [3, 32.0, 0, ..., 0, 1, 0]], dtype=object)

In [4]: target
Out[4]:
array([[ 1.,  0.],
       [ 0.,  1.],
       [ 0.,  1.],
       ...,
       [ 1.,  0.],
       [ 0.,  1.],
       [ 1.,  0.]])



# Specify, compile, and fit the model
model = Sequential()
model.add(Dense(32, activation='relu', input_shape = (n_cols,)))
model.add(Dense(2, activation='softmax'))
model.compile(optimizer='sgd',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
model.fit(predictors, target)

# Calculate predictions: predictions
predictions = model.predict(pred_data)

# Calculate predicted probability of survival: predicted_prob_true
predicted_prob_true = predictions[:,1]

# print predicted_prob_true
print(predicted_prob_true)

script.py> output:
    Epoch 1/10

 32/800 [>.............................] - ETA: 1s - loss: 0.4294 - acc: 0.8125
416/800 [==============>...............] - ETA: 0s - loss: 3.3296 - acc: 0.5913
800/800 [==============================] - 0s - loss: 3.3422 - acc: 0.5813
    Epoch 2/10

 32/800 [>.............................] - ETA: 0s - loss: 1.8684 - acc: 0.5312
384/800 [=============>................] - ETA: 0s - loss: 4.0948 - acc: 0.5495
736/800 [==========================>...] - ETA: 0s - loss: 4.2460 - acc: 0.5829
800/800 [==============================] - 0s - loss: 4.0389 - acc: 0.5850
    Epoch 3/10

 32/800 [>.............................] - ETA: 0s - loss: 2.7468 - acc: 0.8125
416/800 [==============>...............] - ETA: 0s - loss: 4.1272 - acc: 0.6322
800/800 [==============================] - 0s - loss: 4.2259 - acc: 0.6025
    Epoch 4/10

 32/800 [>.............................] - ETA: 0s - loss: 1.7413 - acc: 0.7812
416/800 [==============>...............] - ETA: 0s - loss: 2.5945 - acc: 0.6827
800/800 [==============================] - 0s - loss: 3.5191 - acc: 0.6375
    Epoch 5/10

 32/800 [>.............................] - ETA: 0s - loss: 7.3315 - acc: 0.5312
480/800 [=================>............] - ETA: 0s - loss: 8.5110 - acc: 0.4667
800/800 [==============================] - 0s - loss: 6.7790 - acc: 0.5363
    Epoch 6/10

 32/800 [>.............................] - ETA: 0s - loss: 2.3314 - acc: 0.7812
224/800 [=======>......................] - ETA: 0s - loss: 3.4336 - acc: 0.6741
448/800 [===============>..............] - ETA: 0s - loss: 3.0553 - acc: 0.6540
672/800 [========================>.....] - ETA: 0s - loss: 3.8831 - acc: 0.6265
800/800 [==============================] - 0s - loss: 3.9208 - acc: 0.6187
    Epoch 7/10

 32/800 [>.............................] - ETA: 0s - loss: 6.0905 - acc: 0.5625
544/800 [===================>..........] - ETA: 0s - loss: 4.5016 - acc: 0.5993
800/800 [==============================] - 0s - loss: 4.3683 - acc: 0.6150
    Epoch 8/10

 32/800 [>.............................] - ETA: 0s - loss: 5.4136 - acc: 0.6562
576/800 [====================>.........] - ETA: 0s - loss: 4.4093 - acc: 0.6510
800/800 [==============================] - 0s - loss: 4.4104 - acc: 0.6362
    Epoch 9/10

 32/800 [>.............................] - ETA: 0s - loss: 4.4823 - acc: 0.4375
352/800 [============>.................] - ETA: 0s - loss: 3.7486 - acc: 0.6108
672/800 [========================>.....] - ETA: 0s - loss: 4.3139 - acc: 0.5878
800/800 [==============================] - 0s - loss: 4.3049 - acc: 0.5913
    Epoch 10/10

 32/800 [>.............................] - ETA: 0s - loss: 3.7618 - acc: 0.6875
320/800 [===========>..................] - ETA: 0s - loss: 3.0609 - acc: 0.6469
800/800 [==============================] - 0s - loss: 3.2543 - acc: 0.6475
    [  8.04192241e-05   1.79678686e-02   1.00000000e+00   8.93172204e-01
       1.68145663e-04   4.65810626e-05   2.31857680e-08   8.68677069e-03
       1.19172801e-05   9.99677300e-01   3.26124078e-04   3.50475020e-04
       2.53181588e-05   9.99942303e-01   6.16793623e-05   1.37346058e-06
       1.35837391e-03   4.96428072e-01   1.85129394e-07   9.98506069e-01
       1.00000000e+00   2.83004803e-04   3.63554093e-08   1.26415456e-03
       9.99999404e-01   3.96378273e-05   9.99987364e-01   9.99999642e-01
       5.22786322e-05   9.96197760e-01   1.63896218e-01   9.98909593e-01
       5.43481074e-05   1.01422053e-03   7.80441938e-03   1.00000000e+00
       2.79098097e-03   7.16344148e-05   9.99953628e-01   8.11571851e-02
       3.25243128e-03   3.45960632e-02   1.79600671e-01   7.21107290e-06
       1.27284946e-02   5.18673630e-07   9.99999642e-01   1.01175165e-05
       5.29719353e-01   1.00000000e+00   9.99952197e-01   3.95783504e-13
       8.20287764e-01   9.99983311e-01   1.12873553e-04   1.47294579e-02
       1.00000000e+00   8.69934502e-06   3.55536006e-02   5.43481074e-05
       5.52056326e-06   6.26019808e-03   2.95817663e-05   9.99999762e-01
       3.64947366e-03   4.96782968e-06   1.78762688e-03   9.99610364e-01
       1.12252186e-04   8.87655199e-01   3.26963374e-04   3.84533793e-01
       5.82958501e-06   1.24438742e-07   2.74866335e-02   2.34421510e-02
       5.88031393e-03   4.71504312e-03   6.79147779e-05   9.99971628e-01
       1.91783443e-01   2.20826660e-05   4.42688307e-03   3.66480730e-04
       3.68609850e-04   2.29976233e-03   1.16354926e-03   9.88544762e-01
       1.04574785e-02   4.79899287e-01   2.52051723e-05]




# Import the SGD optimizer
from keras.optimizers import SGD

# Create list of learning rates: lr_to_test
lr_to_test = [.000001, 0.01, 1]

# Loop over learning rates
for lr in lr_to_test:
    print('-------------------\n\n')
    print('10 Epochs for a model with learning rate = %f\n'%lr )

    # Build a new unoptimizel model
    model = get_new_model()

    # Compile with SGD optimizer using specified learning rate
    model.compile(optimizer = SGD(lr=lr), loss = 'categorical_crossentropy')

    # Fit
    model.fit(predictors, target)




# nedan har samma data som ovan.

# Specify
model = Sequential()
n_cols = predictors.shape[1]
input_shape = (n_cols,)
model.add(Dense(100, activation='relu', input_shape = input_shape))
model.add(Dense(100, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile. Use 'accuracy' to see what fraction of predictions are correct
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Fit
model.fit(predictors, target, validation_split = 0.3)



 Train on 623 samples, validate on 268 samples
    Epoch 1/10

 32/623 [>.............................] - ETA: 1s - loss: 3.3028 - acc: 0.4062
352/623 [===============>..............] - ETA: 0s - loss: 1.4383 - acc: 0.5568
623/623 [==============================] - 0s - loss: 1.3099 - acc: 0.6019 - val_loss: 0.6868 - val_acc: 0.7201
    Epoch 2/10

 32/623 [>.............................] - ETA: 0s - loss: 0.6920 - acc: 0.7500
384/623 [=================>............] - ETA: 0s - loss: 0.8502 - acc: 0.5938
623/623 [==============================] - 0s - loss: 0.8847 - acc: 0.5650 - val_loss: 1.1091 - val_acc: 0.6418
    Epoch 3/10

 32/623 [>.............................] - ETA: 0s - loss: 1.0394 - acc: 0.5938
384/623 [=================>............] - ETA: 0s - loss: 0.8181 - acc: 0.6276
623/623 [==============================] - 0s - loss: 0.8011 - acc: 0.6260 - val_loss: 0.8612 - val_acc: 0.6418
    Epoch 4/10

 32/623 [>.............................] - ETA: 0s - loss: 0.6474 - acc: 0.6875
448/623 [====================>.........] - ETA: 0s - loss: 0.7524 - acc: 0.6719
623/623 [==============================] - 0s - loss: 0.7539 - acc: 0.6501 - val_loss: 0.6843 - val_acc: 0.7127
    Epoch 5/10

 32/623 [>.............................] - ETA: 0s - loss: 0.6876 - acc: 0.6250
384/623 [=================>............] - ETA: 0s - loss: 0.6378 - acc: 0.6536
623/623 [==============================] - 0s - loss: 0.6776 - acc: 0.6404 - val_loss: 0.5785 - val_acc: 0.7127
    Epoch 6/10

 32/623 [>.............................] - ETA: 0s - loss: 0.5648 - acc: 0.6875
448/623 [====================>.........] - ETA: 0s - loss: 0.6659 - acc: 0.6518
623/623 [==============================] - 0s - loss: 0.6564 - acc: 0.6517 - val_loss: 0.5257 - val_acc: 0.7500
    Epoch 7/10

 32/623 [>.............................] - ETA: 0s - loss: 0.5568 - acc: 0.7500
480/623 [======================>.......] - ETA: 0s - loss: 0.6118 - acc: 0.6750
623/623 [==============================] - 0s - loss: 0.5997 - acc: 0.6806 - val_loss: 0.5106 - val_acc: 0.7201
    Epoch 8/10

 32/623 [>.............................] - ETA: 0s - loss: 0.6041 - acc: 0.7500
416/623 [===================>..........] - ETA: 0s - loss: 0.5781 - acc: 0.7019
623/623 [==============================] - 0s - loss: 0.5907 - acc: 0.6886 - val_loss: 0.5227 - val_acc: 0.7425
    Epoch 9/10

 32/623 [>.............................] - ETA: 0s - loss: 0.5566 - acc: 0.7188
512/623 [=======================>......] - ETA: 0s - loss: 0.6608 - acc: 0.6582
623/623 [==============================] - 0s - loss: 0.6666 - acc: 0.6613 - val_loss: 0.5669 - val_acc: 0.7015
    Epoch 10/10

 32/623 [>.............................] - ETA: 0s - loss: 0.4835 - acc: 0.7812
544/623 [=========================>....] - ETA: 0s - loss: 0.6167 - acc: 0.6893
623/623 [==============================] - 0s - loss: 0.6133 - acc: 0.6854 - val_loss: 0.5313 - val_acc: 0.7500







# nedan har samma data som ovan. tror alla har samma dataset, because why not.

# Import EarlyStopping
from keras.callbacks import EarlyStopping

# Specify the model
model = Sequential()
n_cols = predictors.shape[1]
shape = (n_cols,)
model.add(Dense(100, activation='relu', input_shape = shape))
model.add(Dense(100, activation='relu'))
model.add(Dense(2, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Define early_stopping_monitor
early_stopping_monitor = EarlyStopping(patience = 2)

# Fit. use high epochs now that we have early stopping
model.fit(predictors, target, epochs = 30, validation_split=0.3, callbacks=[early_stopping_monitor])



   Train on 623 samples, validate on 268 samples
    Epoch 1/30

 32/623 [>.............................] - ETA: 1s - loss: 5.6563 - acc: 0.4688
352/623 [===============>..............] - ETA: 0s - loss: 1.8744 - acc: 0.4972
623/623 [==============================] - 0s - loss: 1.6366 - acc: 0.5666 - val_loss: 1.0826 - val_acc: 0.6642
    Epoch 2/30

 32/623 [>.............................] - ETA: 0s - loss: 1.8338 - acc: 0.4688
576/623 [==========================>...] - ETA: 0s - loss: 0.8375 - acc: 0.6024
623/623 [==============================] - 0s - loss: 0.8329 - acc: 0.6051 - val_loss: 0.5714 - val_acc: 0.7276
    Epoch 3/30

 32/623 [>.............................] - ETA: 0s - loss: 0.8504 - acc: 0.6250
608/623 [============================>.] - ETA: 0s - loss: 0.7040 - acc: 0.6480
623/623 [==============================] - 0s - loss: 0.7139 - acc: 0.6469 - val_loss: 0.5227 - val_acc: 0.7649
    Epoch 4/30

 32/623 [>.............................] - ETA: 0s - loss: 0.9775 - acc: 0.6250
352/623 [===============>..............] - ETA: 0s - loss: 0.6618 - acc: 0.6506
623/623 [==============================] - 0s - loss: 0.6668 - acc: 0.6758 - val_loss: 0.5167 - val_acc: 0.7463
    Epoch 5/30

 32/623 [>.............................] - ETA: 0s - loss: 0.5430 - acc: 0.7812
544/623 [=========================>....] - ETA: 0s - loss: 0.6711 - acc: 0.6379
623/623 [==============================] - 0s - loss: 0.6795 - acc: 0.6388 - val_loss: 0.6424 - val_acc: 0.6940
    Epoch 6/30

 32/623 [>.............................] - ETA: 0s - loss: 0.4589 - acc: 0.8438
608/623 [============================>.] - ETA: 0s - loss: 0.6286 - acc: 0.7089
623/623 [==============================] - 0s - loss: 0.6307 - acc: 0.7063 - val_loss: 0.5728 - val_acc: 0.7239
    Epoch 7/30

 32/623 [>.............................] - ETA: 0s - loss: 0.6635 - acc: 0.6562
608/623 [============================>.] - ETA: 0s - loss: 0.6627 - acc: 0.7039
623/623 [==============================] - 0s - loss: 0.6626 - acc: 0.7030 - val_loss: 0.6653 - val_acc: 0.6679
